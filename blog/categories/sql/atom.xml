<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id></id>
    <title>sql | Matthew Daly&apos;s Blog</title>
    <updated>2019-05-08T20:56:51Z</updated>
    <generator>grunt-blogbuilder https://github.com/matthewbdaly/grunt-blogbuilder</generator>
    <author>
        <name>Matthew Daly</name>
        <email>matthew@matthewdaly.co.uk</email>
        <uri>https://matthewdaly.co.uk</uri>
    </author>
    <link rel="alternate" href="https://matthewdaly.co.uk/blog/categories/sql/"/>
    <subtitle>sql | I&apos;m a web developer in Norfolk. This is my blog...</subtitle>
    <rights>Matthew Daly 2019</rights>
    <entry>
        <title type="html"><![CDATA[How much difference does adding an index to a database table make?]]></title>
        <id>https://matthewdaly.co.uk/blog/2019/03/04/how-much-difference-does-adding-an-index-to-a-database-table-make/</id>
        <link href="https://matthewdaly.co.uk/blog/2019/03/04/how-much-difference-does-adding-an-index-to-a-database-table-make/">
        </link>
        <updated>2019-03-04T21:26:18Z</updated>
        <summary type="html"><![CDATA[<p>For the last few weeks, I’ve been kept busy at work building out a new homepage for the legacy intranet system I maintain. The new homepage is built virtually from scratch with React, and has a completely new set of queries. In addition, I’ve also rebuilt the UI for the navigation to use React too. This has allowed me to bypass a lot of the worst code in the whole code base with the intent to get rid of it once the new home page is live - something I’m very pleased about!</p>
<p>As part of this, I built some new functionality to show items added in the last seven days. This section of the home page can be sorted by several parameters, including popularity. I also added the facility to expand that to 31 days via an AJAX request. However, the AJAX request was painfully slow, often taking 20-30 seconds. Also, the home page was quite slow to load in the first place, and examining the query time in Clockwork indicated that the culprit was the query for the new items.</p>
<p>Further examination of the query behind the new items (both on initial page load and the 31 day AJAX request) indicated that the problem was a join. Last year, one of my first tasks had been to add the facility to record a track for any media item when it was visited. This was accomplished using a polymorphic relationship. While Zend 1 doesn’t have the kind of out-of-the-box support for polymorphic relationships that Laravel has, it’s possible to fake it so I created a <code>tracks</code> table whose columns included <code>trackable_id</code> for the primary key of the tracked object, <code>trackable_type</code> for its class, and <code>user_id</code> for the ID of the user who visited it. Now, I was using that same table to determine the number of times each item had been viewed by joining it on each of the media items, which was the first time it was being read for anything other than a report generated in the admin, and performance was dog slow.</p>
<p>Once I’d established that removing that join from the query removed the performance issue, then it became apparent I was going to need to add an index to the <code>tracks</code> table. The table had got fairly large (low hundreds of thousands), so it had a lot to sort through. As the join used the <code>trackable_id</code> field to join onto the items added, that seemed like a good candidate, so I added the index there.</p>
<p>The results were dramatic, to put it mildly. The initial page load time dropped from 4.44s to 1.29s - around a third of the previous amount. For the AJAX request to fetch the last 31 day’s new items, the results were even more impressive - the loading time dropped from 22.44s to 1.61s. Overall, figuring out which part of the query was causing the poor performance and resolving it took about ten minutes, and resulted in a staggering improvement.</p>
<p>If you don’t have a particularly strong theoretical background with relational databases, knowledge of indices can fall by the wayside somewhat. However, as you can see from this example, if you have a particularly slow query, then adding an index can make a staggering difference, so it’s really worth taking the time to understand a bit more about indices and when they can be useful.</p>
]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-text search with MariaDB]]></title>
        <id>https://matthewdaly.co.uk/blog/2018/05/13/full-text-search-with-mariadb/</id>
        <link href="https://matthewdaly.co.uk/blog/2018/05/13/full-text-search-with-mariadb/">
        </link>
        <updated>2018-05-13T13:55:42Z</updated>
        <summary type="html"><![CDATA[<p>Recently I had the occasion to check out MariaDB’s implementation of full-text search. As it’s a relatively recent arrival in MySQL and MariaDB, it doesn’t seem to get all that much attention. In this post I’ll show you how to use it, with a few Laravel-specific pointers. We’ll be using the default <code>User</code> model in a new Laravel installation, which has columns for <code>name</code> and <code>email</code>.</p>
<p>Our first task is to create the fulltext index, which is necessary to perform the query. Run the following command:</p>
<pre><code class="lang-sql">ALTER TABLE users ADD FULLTEXT (name, email);
</code></pre>
<p>As you can see, we can specify multiple columns in our table to index.</p>
<p>If you’re using Laravel, you’ll want to create the following migration for this:</p>
<pre><code class="lang-php">&lt;?php

use Illuminate\Support\Facades\Schema;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Database\Migrations\Migration;

class AddFulltextIndexForUsers extends Migration
{
    /**
     * Run the migrations.
     *
     * @return void
     */
    public function up()
    {
        DB::statement(&#39;ALTER TABLE users ADD FULLTEXT(name, email)&#39;);
    }

    /**
     * Reverse the migrations.
     *
     * @return void
     */
    public function down()
    {
        DB::statement(&#39;ALTER TABLE users DROP INDEX IF EXISTS name&#39;);
    }
}
</code></pre>
<p>Note that the index is named after the first field passed to it, so when we drop it we refer to it as <code>name</code>. Then, to actually query the index, you should run a command something like this:</p>
<pre><code class="lang-sql">SELECT * FROM users WHERE MATCH(name, email) AGAINST (&#39;jeff&#39; IN NATURAL LANGUAGE MODE);
</code></pre>
<p>Note that <code>NATURAL LANGUAGE MODE</code> is actually the default, so you can leave it off if you wish. We also have to specify the columns to match against.</p>
<p>If you’re using Laravel, you may want to create a reusable local scope for it:</p>
<pre><code class="lang-php">    public function scopeSearch($query, $search)
    {
        if (!$search) {
            return $query;
        }
        return $query-&gt;whereRaw(&#39;MATCH(name, email) AGAINST (?)&#39;, [$search]);
    }
</code></pre>
<p>Then you can call it as follows:</p>
<pre><code class="lang-php">User::search(&#39;jeff&#39;)-&gt;get();
</code></pre>
<p>I personally have noticed that the query using the <code>MATCH</code> keywords seems to be far more performant, with the response time being between five and ten times less than a similar command using <code>LIKE</code>, however this observation isn’t very scientific (plus, we are talking about queries that still run in a fraction of a second). However, if you’re doing a particularly expensive query that currently uses a <code>LIKE</code> statement, it’s possible you may get better results by switching to a <code>MATCH</code> statement. Full-text search probably isn’t all that useful in this context - it’s only once we’re talking about longer text, such as blog posts, that some of the advantages like support for stopwords comes into play.</p>
<p>From what I’ve seen this implementation of full-text search is a lot simpler than in PostgreSQL, which has ups and downs. On the one hand, it’s a lot easier to implement, but conversely it’s less useful - there’s no obvious way to perform a full-text search against joined tables. However, it does seem to be superior to using a <code>LIKE</code> statement, so it’s probably a good fit for smaller sites where something like Elasticsearch would be overkill.</p>
]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using stored procedures in your web app]]></title>
        <id>https://matthewdaly.co.uk/blog/2018/03/10/using-stored-procedures-in-your-web-app/</id>
        <link href="https://matthewdaly.co.uk/blog/2018/03/10/using-stored-procedures-in-your-web-app/">
        </link>
        <updated>2018-03-10T15:10:16Z</updated>
        <summary type="html"><![CDATA[<p>In the last few days I’ve done something I’ve never done before, namely written a stored procedure for a web app. Like most web developers, I know enough about SQL to be able to formulate some fairly complex queries, but I hadn’t really touched on control flow functions or stored procedures, and in my experience they tend to be the province of the dedicated database administrator, not us web devs, who will typically delegate more complex functionality to our application code.</p>
<p>In this case, there were a number of factors influencing my decision to use a stored procedure for this:</p>
<ul>
<li>The application was a legacy application which had been worked on by developers of, shall we say, varying skill levels. As a result the database schema was badly designed, with no prospect of changing it without causing huge numbers of breakages</li>
<li>The query in question was used to generate a complex report that was quite time-consuming, therefore the optimisations from using a stored procedure were worthwhile.</li>
<li>The report required that data be grouped by a set of categories which were stored in a separate table, which meant the table had to be pivoted (transformed from rows to columns), resulting in an incredibly complex dynamic query that had to be constructed on the fly by concatenating different SQL strings. In PostgreSQL, this can be done fairly easily using the <code>crosstab</code> function, but MySQL doesn’t have native support for anything like this.</li>
</ul>
<p>Historically, one issue with using stored procedures has been that it kept business logic out of the application code, meaning they are not stored in version control. However, most modern frameworks provide some support for migrations, and since they are intended to be used to make changes to the database, they are the obvious place to define the stored procedure. This particular application was built with an older framework that didn’t come with migrations, so we’d installed <a href="https://phinx.org/">Phinx</a> to handle those for us. Initially, I defined the stored procedure inside a migration that ran a raw query to create the stored procedure, as in this example:</p>
<pre><code class="lang-php">public function up()
{
   $query = &lt;&lt;&lt;EOF
CREATE PROCEDURE IF NOT EXISTS foo
BEGIN
   SELECT * FROM foo;
END
EOF;
   $this-&gt;execute($query);
}

public function down()
{
   $this-&gt;execute(&#39;DROP PROCEDURE IF EXISTS foo&#39;);
}
</code></pre>
<p>Once this is done, you can then use your framework’s particular support for raw queries to call <code>CALL foo()</code> whenever your stored procedure needs to be executed.</p>
<p>However, we soon ran into an issue. It turns out <code>mysqldump</code> doesn’t export stored procedures by default, so there was a risk that anyone working on the code base might import the database from an SQL file and not get the migrations. I’d used the Symfony Console component to create a simple command-line tool, reminiscent of Laravel’s Artisan, so I used that to create a command to set up the stored procedure, amended the migration to call that command, and placed a check in the application where the procedure was called so that if it was not defined the command would be called and the procedure would be created. In most cases this wouldn’t be an issue.</p>
<p>Having now had experience using stored procedures in a web application, there are a number of issues they raise:</p>
<ul>
<li>It’s hard to make queries flexible, whereas with something like Eloquent it’s straightforward to conditionally apply <code>WHERE</code> statements.</li>
<li>While storing them in migrations is a practical solution, if the database is likely to be imported rather than created from scratch during development it can be problematic.</li>
<li>They aren’t easily portable, not just between database management systems, but between different versions - the production server was using an older version of MySQL, and it failed to create the procedure. It’s therefore good practice for your migrations to check the procedure was created successfully and raise a noisy exception if they failed.</li>
</ul>
<p>Conversely, they do bring certain benefits:</p>
<ul>
<li>For particularly complex transactions that don’t change, such as generating reports, they are a good fit since they reduce the amount of data that needs to be sent to the database and allow the query to be pre-optimised somewhat.</li>
<li>If a particular query is unusually expensive, is called often, and can’t be cached, it may improve performance to make it a stored procedure.</li>
<li>Doing a query in a for loop is usually a very big no-no. However, if there really is no way to avoid it (and this should almost never happen), it would make sense to try to do it in a stored procedure using SQL rather than in application code since that would minimise the overhead.</li>
<li>If multiple applications need to work with the same database, using stored procedures for queries in more than one application removes the need to reimplement or copy over the code for the query in the second application - they can just call the same procedure, and if it needs to be changed it need only be done once.</li>
</ul>
<p>Honestly, I’m not sure I’m ever likely to again come across a scenario where using a stored procedure in a web application would be beneficial, but it’s been very interesting delving into aspects of SQL that I don’t normally touch on and I’ve picked up on some rarely-used SQL statements that I haven’t used before, such as <code>GROUP_CONCAT()</code> and <code>CASE</code>. With the widespread adoption of migrations in most frameworks, I think that the argument that using stored procedures keeps application logic out of version control no longer holds any water, since developers can generally be trusted to store changes to database structure in their migrations and not start messing them around, so the same applies for stored procedures. Report generation seems to be the ideal use case since this invariably involves complex queries that run regularly and don’t change often, and this is where I expect it would be most likely I’d have cause to use them again.</p>
]]></summary>
    </entry>
</feed>